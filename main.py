import json
import pandas as pd
import scraper
import time

# Set the configuration options
enable_group_query = False
enable_tag_search = False
enable_keep_tmp_files = False
query_using_local_db = True
clean_response_file = False
compareHash = True
rehash = True
shutdown_on_pause = False
url = 'https://mb-api.abuse.ch/api/v1/'
queries = 1200
nr_of_search_entries = '1000'
groups = ["RedLineStealer"]


# Define a function to get the groups
def get_groups(groups_list):
    if not groups_list:
        print('No groups string provided. Reading groups from file "groups.txt".')
        with open("groups.txt", "rt") as g:
            groups_list.append(str(g).split(','))
    else:
        print('Reading provided groups string.')
        return groups_list


# Define a function to convert the JSON response to a CSV file
def write():
    print('Opening "response.json" for conversion to csv.')
    with open("response.json", 'r', encoding='utf-8') as f:
        df = pd.read_json(f, lines=True)
    for i in df.index:
        try:
            if df.at[i, 'signature'] in df.at[i, 'tags']:
                df.at[i, 'tags'].remove(str(df.at[i, 'signature']))
            df.at[i, 'downloads'] = df.at[i, 'intelligence']['downloads']
            df.at[i, 'uploads'] = df.at[i, 'intelligence']['uploads']
            df.at[i, 'mail'] = df.at[i, 'intelligence']['mail']
            df.at[i, 'intelligence'] = df.at[i, 'intelligence']['clamav']
        except:
            pass

    # Remove columns you don't need
    df = df.drop(
        ['sha1_hash', 'sha3_384_hash', 'md5_hash', 'reporter', 'anonymous', 'imphash',
            'tlsh', 'telfhash', 'gimphash', 'ssdeep', 'dhash_icon', 'archive_pw', 'file_type_mime', 'comment',
            'file_information', 'ole_information', 'code_sign', 'yara_rules', 'vendor_intel', 'comments', 'mail'], axis=1)

    # Write the updated dataframe to a new CSV file
    df.to_csv('data.csv', index=False)


# Define a function to stop the script
def stop(remaining_queries):
    with open('response.json', 'r', encoding='utf-8') as f:
        print('Total number of completed queries in response.json:', len(f.readlines()))
    if not remaining_queries:
        return False
    elif remaining_queries:
        print('Saving progress')
        scraper.writeHash(remaining_queries)
    if shutdown_on_pause:
        print('Shutting down...')
        exit()
    else:
        time.sleep(2)
        return True


if __name__ == '__main__':
    x = True
    if query_using_local_db:
        enable_group_query = False
        enable_tag_search = False
    get_groups(groups)
    while x:
        x = stop(scraper.getData())
    write()
    print('Done')
